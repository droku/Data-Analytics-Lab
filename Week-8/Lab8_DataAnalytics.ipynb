{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OYeoMBuaLHg7"
   },
   "source": [
    "# Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TJqsF9TDLOEy"
   },
   "source": [
    "### Question 1:  \n",
    "### Max Margin Classifier in 1D\n",
    "**Let's assume two classes in 1D dimension. Class 1 : 9,10,11,12,13,14; Class -1 : -9,-10,-11,-12,-13,-14<br> <br>a) Plot these points. <br><br>b) Are they linearly separable?<br><br>c) Assume a threshold t. Points to one side of the threshold belong to class -1 and points to the other side of the threshold belong to class 1. Find the range of values of t which classifies the two classes completely. <br><br>d) Let's say y_i is the label and can take values -1 or 1. Max margin classifiers aims to find a supporting hyperplane (a boundary between the two classes) such the distance (or margin) from points belonging to either class is maximised. The points closest to the boundary on either side are termed as support vectors. Maximising the perpendicular distance from the support vectors to line will achieve the goal of max margin classifier. Our objective/cost function for the 1D case is cost = max{min(y_i(t-x_i))} where t is the threhold, y_i is the label and x_i is the data point. Plot the cost as a function of the threshold for the range of values of t found in the (c) part. <br><br>e) For what t is the cost maximum? This is the threshold that optimally classifies the two datasets. Now plot the threshold, and the two classes on a graph. Assign appropriate colours to distinguish the two classes and the threshold.<br><br>f) What are the support vectors?<br><br>g) Comment on the threshold. Is it the mid point of the two support vectors in this linearly separable 1D case?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "JH3BYGuoL5qT"
   },
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PK1hkTmcL4Dx"
   },
   "source": [
    "Write your final answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ne_Gmz_sMNeT"
   },
   "source": [
    "### Question 2\n",
    "### Outliers and Slack Parameter\n",
    "**a) Class 1 : -6,9,10,11,12,13,14; Class -1 : -9,-10,-11,-12,-13,-14 Repeat all parts of Q1. Does the boundary shift? Is this more optimal than the classifier in Q1? You will see that there is an outlier. Will it be better to ignore the outlier so that the classifier generalizes better? <br><br> b) Class 1 : -10,9,10,11,12,13,14; Class -1 : -9,-10,-11,-12,-13,-14. Is this data linearly separable? Is there an outlier? Ignore the outlier and then report the threhold.<br><br>c) In the above cases, we allow for some error so that our data generalises better. How much of this error is permissible is captured by a slack parameter.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "szep-f5xMj4v"
   },
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H0UB-g-7MmOH"
   },
   "source": [
    "Write your final answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "74Bz1CVBL0GI"
   },
   "source": [
    "### Question 3 : \n",
    "### SVM in 2D\n",
    "**Using sklearn, visualise the data points and plot the decision boundary for the following 2D datapoints, into 2 classes. Also mention the support vectors for each case. Use sklearn's SVC implementation with 'linear' kernel and default C. <br> <br>\n",
    "(a)  Class 1 - (-1,-2), (1,1), (1.5, 1.5), (-2,-1), (0,0), (1,-0.5) <br> Class 2 - (4.1,5.2), (6,8), (4,4), (7,8), (4.5,6), (5.5,6)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Now add the point (3.4,2.3) to Class 0 from part (a), and classify into Class0 and Class1 again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) Now add the point (4.5,4.5) to Class0 in part (a) and classify into classes 0 and 1 again. <br>  Comment on whether it is possible to perfectly classify these points using a linear classifier in 2D.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change of support vectors\n",
    "**(d) How did the addition of one point in (b) and (c) affect the separating plane in (a)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Now add the point (-1.3,-1.9) to part (a) and classify again. This time, did the decision boundary change?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perturbation of non-support vectors\n",
    "**(f) Now perturb all the points in both classes in part (a) except (1.5,1.5) in class0 and (4,4) in class1 as follows. Add -0.5 to each x and y coordinate in Class0 and 0.5 to each point in Class1. Did the decision boundary change?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) How did the perturbation of points which are not support vectors in (a) affect the final classifier? Did the decision boundary change? Why not?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4  \n",
    "### Introduction of Kernels\n",
    "**a) Plot two concentric circles of radii 4 and 8 with origin at 0. Sample 100 points uniformly from each of these circles. (use rcos(theta) and rsin(theta) to find the coordinates of the points lying on the circle. r is the radius of the circle. Vary theta from 3.6 degrees to 360 degrees in intervals of 3.6 degrees.)<br><br>b) Is the data linearly separable? <br><br>c)Since it's not, we will transform them to a different space (with probably different number of dimensions). The transformation (which generally boils down to inner product - something that we call a kernel) will make the data linearly separable in the different space. For each point, compute features as [1, x, x^2,xy,y^2,y]. This transforms the data from 2D to 6D. Now, use sklearn's SVM classifier (linear kernel) to classify the transformed feature space into two classes. <br><br>d)Now apply SVM classifier with i) polynomial kernel with degree 2  ii) rbf kernel and perform the classification. Visualize the decision boundary in the original 2D space in both the cases. Report the accuracies. Which performs best and why?<br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 \n",
    "### Visualising effect of kernels\n",
    "**Load svm_c0.txt and svm_c1.txt. Create labels as 0 and 1 for these correspondingly. Visualise the two classes and plot decision boundaries obtained by the following kernels and corresponding hyperparameters.<br>(a) Linear kernel with C as 1.0 <br>(b) RBF kernel with gamma as [0.1,1.0,10.0,50.0]** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take Home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the Forest Cover Types dataset from sklearn. 1. Use sklearn's SVM function to classify the data. Do hyperparameter tuning using <br><br> a) C = {0.001,0.01,0.1,1,10,100} <br><br> b) kernel = {'linear','poly','rbf','sigmoid'}. For the polynomial kernel, test with degrees 1,2,3,4,5. For the other kernels, test with gamma values of 0.001,0.01,0.1,1,10,100. <br><br> To do the above exercise of hyperparameter tuning, use the cross validation function in sklearn. Do not perform each experiment separately. <br><br> c) Report the accuracies (F1 scores) of the 5 best cases in a tabular form. <br><br>  d) Plot the confusion matrices of the best 5 cases.<br> e) Comment on the significance of each hyperparameter. <br><br> 2. a) Use sklearn's logistic regression, KNN and Naive Bayes to classify the above dataset. Report the F1 scores and confusion matrices in each case. <br> b) Arrange the 4 classifiers (LR, NB, SVM, KNN) in the decreasing order of F1 scores. Comment on which classifier performs best and why.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab8_DataAnalytics.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
